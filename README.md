# English

# NSFWDetectorKit

A small Swift Package wrapping a Core ML model for **NSFW (Not Safe For Work)** content detection.  
ğŸ‘‰ It allows you to classify images as **explicit (unsafe)** or **safe** based on a configurable threshold.

---

## ğŸ“¦ Installation (SPM)

1. Xcode â†’ **File â–¸ Add Packagesâ€¦**  
2. Repository URL:
   ```
   https://github.com/mehmettirpan/NSFWDetectorKit.git
   ```
3. Dependency Rule: *Up to Next Major Version* (recommended)  
4. Import codes:
   ```swift
   import NSFWDetectorKit
   ```

---

## âš™ï¸ Requirements
- iOS 15.0+  
- Xcode 15+  
- Swift 5.9+  
- Core ML model file included: `NSFWDetector.mlpackage`

---

## ğŸš€ Usage

### Classify with UIImage
```swift
import NSFWDetectorKit

let image: UIImage = ... // e.g. from photo library

CoreMLNSFWScanner.shared.classify(image) { result in
    switch result {
    case .success(let (score, labels)):
        print("NSFW probability =", score)
        print("All labels =", labels)
    case .failure(let error):
        print("Error:", error)
    }
}
```

### Classify with Threshold
```swift
let threshold: Float = 0.20 // adjust as you wish

CoreMLNSFWScanner.shared.classify(image, threshold: threshold) { result in
    switch result {
    case .success(let (blocked, score, labels)):
        print("NSFW probability =", score)
        print("Blocked? =", blocked ? "âŒ Not allowed" : "âœ… Allowed")
    case .failure(let error):
        print("Error:", error)
    }
}
```

### Classify with CGImage (UIKit-free)
```swift
let cg: CGImage = ...
CoreMLNSFWScanner.shared.classify(
    cgImage: cg,
    orientation: .up
) { result in
    switch result {
    case .success(let (score, labels)):
        print("NSFW probability =", score)
        print("Labels =", labels)
    case .failure(let error):
        print("Error:", error)
    }
}
```

## ğŸ“± Demo App

If you want to test this Swift Package with a simple UI application, you can check out the demo project here:

ğŸ‘‰ [NSFWDetector-Test](https://github.com/mehmettirpan/NSFWDetector-Test.git)

This project provides a basic interface to load images and run the NSFW classification in real-time, making it easier to experiment with thresholds and outputs.

---

## ğŸ“ Notes
- `score` range: `0 and below` (safe) â†’ `1.0` (explicit).  
- The threshold is entirely up to your application:
  - **0.20** â†’ strict (blocks most borderline content)
  - **0.50** â†’ balanced
  - **0.80** â†’ lenient  
- It is recommended to run an additional control mechanism in the range **0.15â€“0.60** to catch borderline cases.
- Performance depends on device hardware (Apple Neural Engine accelerates).

---

### ğŸ” Threshold Recommendation
- Model outputs may sometimes produce values below 0.0. Treat any value **< 0.0** as `0.0` (safe).  
- In testing, a **threshold of 0.20** gave the best balance:
  - `< 0.20` â†’ Safe (publishable)
  - `â‰¥ 0.20` â†’ Explicit (should be blocked)
- You can experiment with different thresholds depending on your use case.

---

## ğŸ“‚ Project Structure
```
Package.swift
Sources/NSFWDetectorKit/
 â”œâ”€â”€ CoreMLNSFWScanner.swift
 â”œâ”€â”€ NSFWDetectorKit.swift
 â””â”€â”€ NSFWDetector.mlpackage
```

---

## ğŸ“„ License
MIT â€” see [LICENSE](LICENSE).

# TÃ¼rkÃ§e

# NSFWDetectorKit

Core ML tabanlÄ± bir **NSFW (MÃ¼stehcen Ä°Ã§erik)** tespit modeli saran kÃ¼Ã§Ã¼k bir Swift Paketi.  
ğŸ‘‰ GÃ¶rselleri, ayarlanabilir bir eÅŸik deÄŸerine gÃ¶re **mÃ¼stehcen (yayÄ±nlanamaz)** veya **gÃ¼venli (yayÄ±nlanabilir)** olarak sÄ±nÄ±flandÄ±rmanÄ±za olanak saÄŸlar.

---

## ğŸ“¦ Kurulum (SPM)

1. Xcode â†’ **File â–¸ Add Packagesâ€¦**  
2. Depo URL'si:
   ```
   https://github.com/mehmettirpan/NSFWDetectorKit.git
   ```
3. BaÄŸÄ±mlÄ±lÄ±k KuralÄ±: *Up to Next Major Version* (Ã¶nerilir)  
4. Koddaki import:
   ```swift
   import NSFWDetectorKit
   ```

---

## âš™ï¸ Gereksinimler
- iOS 15.0+  
- Xcode 15+  
- Swift 5.9+  
- Core ML model dosyasÄ± dahil: `NSFWDetector.mlpackage`

---

## ğŸš€ KullanÄ±m

### UIImage ile SÄ±nÄ±flandÄ±rma
```swift
import NSFWDetectorKit

let image: UIImage = ... // Ã¶rn. fotoÄŸraf kÃ¼tÃ¼phanesinden

CoreMLNSFWScanner.shared.classify(image) { result in
    switch result {
    case .success(let (score, labels)):
        print("NSFW olasÄ±lÄ±ÄŸÄ± =", score)
        print("TÃ¼m etiketler =", labels)
    case .failure(let error):
        print("Hata:", error)
    }
}
```

### EÅŸik DeÄŸeri ile SÄ±nÄ±flandÄ±rma
```swift
let threshold: Float = 0.20 // istediÄŸiniz gibi ayarlayÄ±n

CoreMLNSFWScanner.shared.classify(image, threshold: threshold) { result in
    switch result {
    case .success(let (blocked, score, labels)):
        print("NSFW olasÄ±lÄ±ÄŸÄ± =", score)
        print("Engellendi mi? =", blocked ? "âŒ Ä°zin verilmedi" : "âœ… Ä°zin verildi")
    case .failure(let error):
        print("Hata:", error)
    }
}
```

### CGImage ile SÄ±nÄ±flandÄ±rma (UIKit gerektirmez)
```swift
let cg: CGImage = ...
CoreMLNSFWScanner.shared.classify(
    cgImage: cg,
    orientation: .up
) { result in
    switch result {
    case .success(let (score, labels)):
        print("NSFW olasÄ±lÄ±ÄŸÄ± =", score)
        print("Etiketler =", labels)
    case .failure(let error):
        print("Hata:", error)
    }
}
```

## ğŸ“± Demo UygulamasÄ±

Bu Swift Package'i basit bir arayÃ¼z ile test etmek isterseniz demo projeye gÃ¶z atabilirsiniz:

ğŸ‘‰ [NSFWDetector-Test](https://github.com/mehmettirpan/NSFWDetector-Test.git)

Bu proje, gÃ¶rselleri yÃ¼kleyip NSFW sÄ±nÄ±flandÄ±rmasÄ±nÄ± gerÃ§ek zamanlÄ± Ã§alÄ±ÅŸtÄ±rabileceÄŸiniz basit bir arayÃ¼z sunar. BÃ¶ylece eÅŸik deÄŸerlerini ve Ã§Ä±ktÄ±larÄ±nÄ± kolayca deneyebilirsiniz.

---

## ğŸ“ Notlar
- `score` aralÄ±ÄŸÄ±: `0 ve altÄ±` (gÃ¼venli) â†’ `1.0` (mÃ¼stehcen).  
- EÅŸik tamamen uygulamaya baÄŸlÄ±dÄ±r:
  - **0.20** â†’ sÄ±kÄ± (borderline iÃ§eriklerin Ã§oÄŸunu engeller)
  - **0.50** â†’ dengeli
  - **0.80** â†’ esnek  
- **0.15â€“0.60** aralÄ±ÄŸÄ±nda ek bir kontrol mekanizmasÄ± Ã§alÄ±ÅŸtÄ±rÄ±lmasÄ± sÄ±nÄ±rdaki durumlarÄ± yakalamak iÃ§in Ã¶nerilir.
- Performans cihaz donanÄ±mÄ±na baÄŸlÄ±dÄ±r (Apple Neural Engine hÄ±zlandÄ±rÄ±r).

### ğŸ” EÅŸik Ã–nerisi
- Model Ã§Ä±ktÄ±larÄ± bazen 0.0'Ä±n altÄ±nda deÄŸerler Ã¼retebilir. Herhangi bir **< 0.0** deÄŸeri `0.0` (gÃ¼venli) olarak deÄŸerlendirin.  
- Testlerde, **0.20 eÅŸik deÄŸeri** en iyi dengeyi saÄŸladÄ±:
  - `< 0.20` â†’ GÃ¼venli (yayÄ±nlanabilir)
  - `â‰¥ 0.20` â†’ MÃ¼stehcen (engellenmeli)
- Ä°htiyacÄ±nÄ±za gÃ¶re farklÄ± eÅŸik deÄŸerleri deneyebilirsiniz.

---

## ğŸ“‚ Proje YapÄ±sÄ±
```
Package.swift
Sources/NSFWDetectorKit/
 â”œâ”€â”€ CoreMLNSFWScanner.swift
 â”œâ”€â”€ NSFWDetectorKit.swift
 â””â”€â”€ NSFWDetector.mlpackage
```

---

## ğŸ“„ Lisans
MIT â€” bkz. [LICENSE](LICENSE).
