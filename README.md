# English

# NSFWDetectorKit

A small Swift Package wrapping a Core ML model for **NSFW (Not Safe For Work)** content detection.  
üëâ It allows you to classify images as **explicit (unsafe)** or **safe** based on a configurable threshold.

---

## üì¶ Installation (SPM)

1. Xcode ‚Üí **File ‚ñ∏ Add Packages‚Ä¶**  
2. Repository URL:
   ```
   https://github.com/mehmettirpan/NSFWDetectorKit.git
   ```
3. Dependency Rule: *Up to Next Major Version* (recommended)  
4. Import codes:
   ```swift
   import NSFWDetectorKit
   ```

---

## ‚öôÔ∏è Requirements
- iOS 15.0+  
- Xcode 15+  
- Swift 5.9+  
- Core ML model file included: `NSFWDetector.mlpackage`

---

## üöÄ Usage

### Classify with UIImage
```swift
import NSFWDetectorKit

let image: UIImage = ... // e.g. from photo library

CoreMLNSFWScanner.shared.classify(image) { result in
    switch result {
    case .success(let (score, labels)):
        print("NSFW probability =", score)
        print("All labels =", labels)
    case .failure(let error):
        print("Error:", error)
    }
}
```

### Classify with Threshold
```swift
let threshold: Float = 0.20 // adjust as you wish

CoreMLNSFWScanner.shared.classify(image, threshold: threshold) { result in
    switch result {
    case .success(let (blocked, score, labels)):
        print("NSFW probability =", score)
        print("Blocked? =", blocked ? "‚ùå Not allowed" : "‚úÖ Allowed")
    case .failure(let error):
        print("Error:", error)
    }
}
```

### Classify with CGImage (UIKit-free)
```swift
let cg: CGImage = ...
CoreMLNSFWScanner.shared.classify(
    cgImage: cg,
    orientation: .up
) { result in
    switch result {
    case .success(let (score, labels)):
        print("NSFW probability =", score)
        print("Labels =", labels)
    case .failure(let error):
        print("Error:", error)
    }
}
```

---

## üìù Notes
- `score` range: `0 and below` (safe) ‚Üí `1.0` (explicit).  
- The threshold is entirely up to your application:
  - **0.20** ‚Üí strict (blocks most borderline content)
  - **0.50** ‚Üí balanced
  - **0.80** ‚Üí lenient  
- It is recommended to run an additional control mechanism in the range **0.15‚Äì0.60** to catch borderline cases.
- Performance depends on device hardware (Apple Neural Engine accelerates).

---

### üîé Threshold Recommendation
- Model outputs may sometimes produce values below 0.0. Treat any value **< 0.0** as `0.0` (safe).  
- In testing, a **threshold of 0.20** gave the best balance:
  - `< 0.20` ‚Üí Safe (publishable)
  - `‚â• 0.20` ‚Üí Explicit (should be blocked)
- You can experiment with different thresholds depending on your use case.

---

## üìÇ Project Structure
```
Package.swift
Sources/NSFWDetectorKit/
 ‚îú‚îÄ‚îÄ CoreMLNSFWScanner.swift
 ‚îú‚îÄ‚îÄ NSFWDetectorKit.swift
 ‚îî‚îÄ‚îÄ NSFWDetector.mlpackage
```

---

## üìÑ License
MIT ‚Äî see [LICENSE](LICENSE).

# T√ºrk√ße

# NSFWDetectorKit

Core ML tabanlƒ± bir **NSFW (M√ºstehcen ƒ∞√ßerik)** tespit modeli saran k√º√ß√ºk bir Swift Paketi.  
üëâ G√∂rselleri, ayarlanabilir bir e≈üik deƒüerine g√∂re **m√ºstehcen (yayƒ±nlanamaz)** veya **g√ºvenli (yayƒ±nlanabilir)** olarak sƒ±nƒ±flandƒ±rmanƒ±za olanak saƒülar.

---

## üì¶ Kurulum (SPM)

1. Xcode ‚Üí **File ‚ñ∏ Add Packages‚Ä¶**  
2. Depo URL'si:
   ```
   https://github.com/mehmettirpan/NSFWDetectorKit.git
   ```
3. Baƒüƒ±mlƒ±lƒ±k Kuralƒ±: *Up to Next Major Version* (√∂nerilir)  
4. Koddaki import:
   ```swift
   import NSFWDetectorKit
   ```

---

## ‚öôÔ∏è Gereksinimler
- iOS 15.0+  
- Xcode 15+  
- Swift 5.9+  
- Core ML model dosyasƒ± dahil: `NSFWDetector.mlpackage`

---

## üöÄ Kullanƒ±m

### UIImage ile Sƒ±nƒ±flandƒ±rma
```swift
import NSFWDetectorKit

let image: UIImage = ... // √∂rn. fotoƒüraf k√ºt√ºphanesinden

CoreMLNSFWScanner.shared.classify(image) { result in
    switch result {
    case .success(let (score, labels)):
        print("NSFW olasƒ±lƒ±ƒüƒ± =", score)
        print("T√ºm etiketler =", labels)
    case .failure(let error):
        print("Hata:", error)
    }
}
```

### E≈üik Deƒüeri ile Sƒ±nƒ±flandƒ±rma
```swift
let threshold: Float = 0.20 // istediƒüiniz gibi ayarlayƒ±n

CoreMLNSFWScanner.shared.classify(image, threshold: threshold) { result in
    switch result {
    case .success(let (blocked, score, labels)):
        print("NSFW olasƒ±lƒ±ƒüƒ± =", score)
        print("Engellendi mi? =", blocked ? "‚ùå ƒ∞zin verilmedi" : "‚úÖ ƒ∞zin verildi")
    case .failure(let error):
        print("Hata:", error)
    }
}
```

### CGImage ile Sƒ±nƒ±flandƒ±rma (UIKit gerektirmez)
```swift
let cg: CGImage = ...
CoreMLNSFWScanner.shared.classify(
    cgImage: cg,
    orientation: .up
) { result in
    switch result {
    case .success(let (score, labels)):
        print("NSFW olasƒ±lƒ±ƒüƒ± =", score)
        print("Etiketler =", labels)
    case .failure(let error):
        print("Hata:", error)
    }
}
```

---

## üìù Notlar
- `score` aralƒ±ƒüƒ±: `0 ve altƒ±` (g√ºvenli) ‚Üí `1.0` (m√ºstehcen).  
- E≈üik tamamen uygulamaya baƒülƒ±dƒ±r:
  - **0.20** ‚Üí sƒ±kƒ± (borderline i√ßeriklerin √ßoƒüunu engeller)
  - **0.50** ‚Üí dengeli
  - **0.80** ‚Üí esnek  
- **0.15‚Äì0.60** aralƒ±ƒüƒ±nda ek bir kontrol mekanizmasƒ± √ßalƒ±≈ütƒ±rƒ±lmasƒ± sƒ±nƒ±rdaki durumlarƒ± yakalamak i√ßin √∂nerilir.
- Performans cihaz donanƒ±mƒ±na baƒülƒ±dƒ±r (Apple Neural Engine hƒ±zlandƒ±rƒ±r).

### üîé E≈üik √ñnerisi
- Model √ßƒ±ktƒ±larƒ± bazen 0.0'ƒ±n altƒ±nda deƒüerler √ºretebilir. Herhangi bir **< 0.0** deƒüeri `0.0` (g√ºvenli) olarak deƒüerlendirin.  
- Testlerde, **0.20 e≈üik deƒüeri** en iyi dengeyi saƒüladƒ±:
  - `< 0.20` ‚Üí G√ºvenli (yayƒ±nlanabilir)
  - `‚â• 0.20` ‚Üí M√ºstehcen (engellenmeli)
- ƒ∞htiyacƒ±nƒ±za g√∂re farklƒ± e≈üik deƒüerleri deneyebilirsiniz.

---

## üìÇ Proje Yapƒ±sƒ±
```
Package.swift
Sources/NSFWDetectorKit/
 ‚îú‚îÄ‚îÄ CoreMLNSFWScanner.swift
 ‚îú‚îÄ‚îÄ NSFWDetectorKit.swift
 ‚îî‚îÄ‚îÄ NSFWDetector.mlpackage
```

---

## üìÑ Lisans
MIT ‚Äî bkz. [LICENSE](LICENSE).
